{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5841a012",
   "metadata": {},
   "source": [
    "# Report of Project 1: Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef3e34",
   "metadata": {},
   "source": [
    "## Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f200d",
   "metadata": {},
   "source": [
    "The algorithm used in this project is a __dueling double DQN__ (DuelingDDQN) agent, which is described in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcef16",
   "metadata": {},
   "source": [
    "The DuelingDDQN algorithm refers to a class of RL algorithms called temporal difference (TD) learning. It is a _model-free_ algorithm, because there is no prior planning or implementation of a (stochastic) model of the environment. It is an _off-policy_ algorithm, because it improves a policy different from the policy used to generate the data. While the definition of model-free should be pretty straight forward, the off-policy part may need some some further explanation and what is the definition of a policy after all? \n",
    "\n",
    "Simply put, a policy is a function (incorporating a neural network in our case) which maps states to actions. __The policy__ used in this project is $\\epsilon$-greedy, which selects actions either uniformly at random or based on the learned policy (i.e. a Q-network). The choice between these two is based on $\\epsilon$ which is decayed on each episode by a certain factor, i.e. favoring exploration in the beginning of the learning process and exploitation of the learned policy towards the end. The term off-policy refers to the fact that we use two disinct networks (same architecture but different weight-instances) for a) generating the next step's action (value) and b) updating the policy.\n",
    "\n",
    "Every single step taken by the agent in the environment yields a so-called _experience_, which comprises a _state_ vector (e.g. representing a ray-based perception of the environment), the taken _action_ (based on the policy described above), a reward returned by the environment, the _next state_ of the agent and an indicator if the episode has finished. Each experience is stored in a fixed-size __experience replay buffer__ (a double-ended queue), from which the algorithm samples batches _uniformly at random_ to fit the neural network(s). This breaks the temporal correlation of experiences and thus generates i.i.d. samples from the buffer, which is expected by gradient-based optimizers such as Adam, RMSProp and others.\n",
    "\n",
    "The model-fitting process (i.e. update of the neural network's weights) is done every n-th step of the agent, using a sampled batch of experiences. The process is defined by the name __double DQN__, which is made up by a vanilla _DQN_, i.e. two neural networks, the online network which is continuously updated, and the target network, which is used to generate the next step's action value. The extension _double_ refers to the fact that the _online network_ is used to choose the action (by the highest value) from the next state, as opposed to the vanilla DQN algorithm alone, where the target network is responsible for determining the max. actions _and_ it's value. \n",
    "\n",
    "The update step looks as follows: for all experiences in a given batch we compute the expected action values of the next state's from the online and target network, and estimates of the actual state's values from the online network. Since the experiences also contain the rewards, we can compute the gradient update as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46f348",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\theta_i}L_i(\\theta_i) = \\mathbb{E}_{(s,a,r,s')\\sim U(D)} \\Big[\\big(r+\\gamma Q(s',\\underset{a'}{\\operatorname{argmax}} Q(s',a';\\theta_i);\\theta^-) - Q(s,a;\\theta_i))\\nabla_{\\theta_i}Q(s,a;\\theta_i\\big)\\Big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17094bbb",
   "metadata": {},
   "source": [
    "The estimated Q-values from the target (multiplied by a constant factor $\\gamma$ and added reward $r$), and the local Q-value estimate, $Q(s,a;\\theta_i)$, are passed on the loss function, where the `SmoothL1Loss` a.k.a. Huber-Loss has been chosen, which is more robust to extreme values, i.e. mitigating very large gradients which potentially lead to oszillations and thus slow convergence, especially in the beginning of the learning process, as the network policy is essentially chasing a moving target.\n",
    "\n",
    "The final step in the model-fitting process is to \"soft-update\" the target network's weights. In contrast to overwriting the target-network weights with the online network weights every N time steps, we use __Polyak Averaging__, which updates the weights more often by mixing the weights with tiny bits of both networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b8e2b",
   "metadata": {},
   "source": [
    "$$\\theta_i^- = \\tau \\theta_i + (1-\\tau)\\theta_i^-$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a190e0",
   "metadata": {},
   "source": [
    "The neural network architecture itelf is defined as __dueling DQN__, which main purpose is __sample efficiency__. The main difference to the vanilla DQN is that the layer before the output is split into two streams, where the first stream represents the state-value function $V(s)$ and the second stream represents the action-advantage function $A(s,a)$. \n",
    "\n",
    "Finally, reconstruction of the action-value function $Q(s,a)$ is done by combining the output of the two stream in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9a73cf",
   "metadata": {},
   "source": [
    "$$Q(s,a;\\theta,\\alpha,\\beta) = V(s;\\theta,\\beta) + \\Big(A(s,a;\\theta,\\alpha) - \\frac{1}{|A|}\\underset{a'} \\sum A(a,s';\\theta,\\alpha) \\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f7bc3",
   "metadata": {},
   "source": [
    "In the equation above, $\\theta$ represents the weights of the shared upstream hidden layers, $\\beta$ the weights of the value-function stream and $\\alpha$ the weights of the action-advantage-function stream. The rightmost term in that equation subtracts the mean of the aggregated action-value function, shifting the estimates of the action-advantage stream in order to _stabilize the optimization_ while keeping the relative rank of $A(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c0ccd",
   "metadata": {},
   "source": [
    "Completing this fairly concise explanation of the algorithm (with great support of the outstanding references listed below), the following table provides a listing of hyper-parameters used for the final results presented in the next section:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98777977",
   "metadata": {},
   "source": [
    "Parameter Name | Value | Description\n",
    ":--- | --- | :---\n",
    "hidden_layer_dimensions | (512, 256) | dimensions of neural network hidden layers.\n",
    "activation_fn | ReLU | the activation function used for the neural network hidden layers.\n",
    "buffer_size | 100_000 | replay buffer size.\n",
    "batch_size | 64 | mini-batch size.\n",
    "gamma | 0.99 | discount factor.\n",
    "tau | 1e-3 | interpolation parameter for target-network weight update.\n",
    "lr | 5e-4 | learning rate.\n",
    "update_every | 4 | how often (time steps) to update the network.\n",
    "n_episodes | 2000 | maximum number of training episodes.\n",
    "max_t | 1000 |  maximum number of time steps per episode.\n",
    "eps_start | 1.0 | starting value of epsilon, for epsilon-greedy action selection.\n",
    "eps_end | 0.01 | minimum value of epsilon.\n",
    "eps_decay | 0.995 | multiplicative factor (per episode) for decreasing epsilon.\n",
    "scores_window_length | 100 | length of scores window to monitor convergence.\n",
    "average_target_score | 13.0 | average target score for scores_window_length at which learning stops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c399944",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cd4f8",
   "metadata": {},
   "source": [
    "While the evironment could have been solved within ~600 episodes by using a vanilla DQN, extending the algorithm using a dueling architecture, the environment could be __solved in 477 episodes__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b45cbb",
   "metadata": {},
   "source": [
    "![Scores](scores.png \"Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323d950",
   "metadata": {},
   "source": [
    "## Ideas for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c76f4",
   "metadata": {},
   "source": [
    "From a modeling point of view, there are many ways to improve the algorithms and models even further. A (incomplete) listing of possible future improvements/extensions:\n",
    "\n",
    "* Prioritized Experience Replay (PER)\n",
    "* Distributional DQN\n",
    "* Noisy DQN\n",
    "* Rainbow DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a9be73",
   "metadata": {},
   "source": [
    "Also, it would be interesting to see how Bayesian variants of the given neural network architectures (e.g. Variational Inference or Bayesian approximation using Dropout) could possibly improve decision making, based on posterior-predictive distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5774d",
   "metadata": {},
   "source": [
    "Utilize hyper-parameter optimization frameworks (hyperopt or scikit-optimize) to further optimize the agent's settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da75a4",
   "metadata": {},
   "source": [
    "From an architectural perspective, loose coupling between the `DuelingDDQN` agent, the `DuelingDenseQNetwork`s, `ReplayBuffer`, `Adam` optimizer, `SmoothL1Loss`, etc. should be preferred to enable composing and testing different implementations without changing the code. Moreover, the $\\epsilon$-greedy policy code should be moved into it's own class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65276415",
   "metadata": {},
   "source": [
    "## Additional References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b75bd",
   "metadata": {},
   "source": [
    "* [Miguel Morales, \"Grokking Deep Reinforcement Learning\" Manning Publications.](https://www.manning.com/books/grokking-deep-reinforcement-learning)\n",
    "* [Sutton, Barto \"Reinforcement Learning, 2nd edition\" MIT Press.](https://s3-us-west-1.amazonaws.com/udacity-drlnd/bookdraft2018.pdf)\n",
    "* [Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature518.7540 (2015): 529.]( http://www.davidqiu.com:8888/research/nature14236.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba44096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
