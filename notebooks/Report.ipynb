{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5841a012",
   "metadata": {},
   "source": [
    "# Report of Project 1: Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef3e34",
   "metadata": {},
   "source": [
    "## Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f200d",
   "metadata": {},
   "source": [
    "The agent used in this project is a __DuelingDDQN__, which is described in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae28f8",
   "metadata": {},
   "source": [
    "The DuelingDDQN algorithm refers to a class of RL algorithms called a temporal difference (TD) learning. It is a _model-free_ algorithm, because there is no prior planning or implementation of a (stochastic) model of the environment. It is an _off-policy_ algorithm, because it improves a policy different from the policy used to generate the data.\n",
    "\n",
    "While the definition of model-free should be pretty straight forward, the off-policy part may need some some further explanation and what is the definition of a policy after all? Simply put, a policy is a function which maps states to actions. It could be random to explore the environment and/or deterministic (sort of), i.e. exploiting what it already learned from the environment, aiming to achive the highest possible reward.\n",
    "\n",
    "__The policy__ used here is called $\\epsilon$-greedy, which selects actions either randomly or based on the learned policy (i.e. a Q-network). The choice between these two is based on $\\epsilon$ which is decayed on each episode by a certain factor, i.e. favoring exploration in the beginning of the learning process and exploitation of the learned policy towards the end. The term off-policy refers to the fact that we use two different policies (Q-networks) for a) generating the next step's action (value) and b) updating the policy.\n",
    "\n",
    "Every single step taken by the agent in the environment is called an _experience_, which consists of a _state_ vector (e.g. representing a ray-based perception of the environment), a taken _action_ (based on the policy described above), a reward returned by the environment, the _next state_ of the agent and an indicator if the episode has finished. Each experience is stored in a fixed-size __experience replay buffer__ (a double-ended queue), from which the algorithm samples batches _uniformly at random_ to fit the neural network(s), which generates i.i.d. samples from the buffer, which is expected by gradient-based optimizers such as Adam, RMSProp and others.\n",
    "\n",
    "The model-fitting process (i.e. update of the NN weights) is done every n-th step of the agent, using a sampled batch of experiences. This process is defined by the __double DQN__ algorithm, which consists of two neural networks, the online network which is continuously updated, and the target network, which is used to generate the next step's action value. The extension _double_ refers to the fact that the online network is used to choose the action (with the highest value) from the next state, as opposed to the \"classic\" DQN algorithm. \n",
    "\n",
    "An update step looks as follows: for all experiences in a given batch we compute the expected action values of the next state's from the target network and estimates of the actual state values from the online network. Since the experiences also contain the rewards, we can compute the gradient update, which is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171cefa",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\theta_i}L_i(\\theta_i) = \\mathbb{E}_{(s,a,r,s')\\sim U(D)} \\Big[\\big(r+\\gamma Q(s',\\underset{a'}{\\operatorname{argmax}} Q(s',a';\\theta_i);\\theta^-) - Q(s,a;\\theta_i))\\nabla_{\\theta_i}Q(s,a;\\theta_i\\big)\\Big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c8adf4",
   "metadata": {},
   "source": [
    "The values for the target Q-value estimate $r+\\gamma Q(s',\\underset{a'}{\\operatorname{argmax}} Q(s',a';\\theta_i);\\theta^-)$ and the local Q-value estimate $Q(s,a;\\theta_i)$ are passed on the loss function, where the `SmoothL1Loss` a.k.a Huber Loss has been chosen, which is more robust to extreme values, i.e. not producing very large gradients potentially leading to slow convergence especially in the beginning of the learning process, as the network policy is chasing a moving target.\n",
    "\n",
    "The final step in the model-fitting process is to \"soft-update\" the target network's weights. In contrast to overwriting the target-network weights with the online network weights every N time steps, we use __Polyak Averaging__, which updates the weights more often by mixing the weights with tiny bits of both networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e010414b",
   "metadata": {},
   "source": [
    "$$\\theta_i^- = \\tau \\theta_i + (1-\\tau)\\theta_i^-$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684e993",
   "metadata": {},
   "source": [
    "The neural network architecture is a __Dueling DQN__, where the main difference to the vanilla DQN is that the layer before the output is split into two streams, where the first stream represents the state-value function $V(s)$ and the second stream represents the action-advantage function $A(s,a)$. The whole purpose of that is __sample efficiency__.\n",
    "\n",
    "TODO elaborate more on that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c399944",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cd4f8",
   "metadata": {},
   "source": [
    "While the evironment could already be solved within ~600 episodes using a simple DQN, by extending the algorithm using a dueling architecture, the environment could be __solved in 477 episodes__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b45cbb",
   "metadata": {},
   "source": [
    "![Scores](scores.png \"Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323d950",
   "metadata": {},
   "source": [
    "## Ideas for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da75a4",
   "metadata": {},
   "source": [
    "From an architectural perspective, loose coupling between the `DuelingDDQN` agent, the `DuelingDenseQNetwork`s, `ReplayBuffer`, `Adam` optimizer, `SmoothL1Loss`, etc. should be preferred to enable composing and testing different implementations without changing the code. \n",
    "\n",
    "Moreover, the action-selection code should be moved into it's own class, decoupling the code even further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c76f4",
   "metadata": {},
   "source": [
    "From a modeling point of view, there are also many ways to improve the algorithms and models even further. A (incomplete) list of possible future improvements/extensions are:\n",
    "\n",
    "* Implement Prioritized Experience Replay (PER)\n",
    "* Distributional DQN\n",
    "* Noisy DQN\n",
    "* Rainbow DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a9be73",
   "metadata": {},
   "source": [
    "Also, it would be interesting to see how Bayesian variants (BNNs) of the neural network architecutes could possibly improve decision making, based on it's posterior-predictive distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5774d",
   "metadata": {},
   "source": [
    "Instead of manually testing different hyper-parameter settings, utilization of hyper-parameter optimization tools should be done (e.g. using bayesian optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65276415",
   "metadata": {},
   "source": [
    "## Additional References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b75bd",
   "metadata": {},
   "source": [
    "* [Miguel Morales, \"Grokking Deep Reinforcement Learning\" Manning Publications.](https://www.manning.com/books/grokking-deep-reinforcement-learning)\n",
    "* [Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature518.7540 (2015): 529.]( http://www.davidqiu.com:8888/research/nature14236.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba44096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
